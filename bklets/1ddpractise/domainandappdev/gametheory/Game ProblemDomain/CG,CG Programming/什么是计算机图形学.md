什么是图形，图形编程及对应的game VR/SR编程
=============





什么是图形学
-------------

为了要在PC上展现3D图形,起码要完成以下几个方面的任务:

(假设我们已完成基本的2D图形学系统 – 这些在前面的平台部分的图形系统部分有述,并以这个起点进行工作)

对于建模艺术家,如何表示3d图形? 对于渲染器,如何处理它?怎么样处理更有效率? 对于硬件与硬件厂商,如何与硬件加速合作? 对于用户(各行各业使用图形产品的人),这些图形够VR吗?可交互吗? 对于程序员,如何编程支持以上吗?层次如何?在三角形级,还是在顶点级?

我们一一看来. 如何表示3d图形?首先,现实世界中的物体和模型都是很复杂的.有刚体,流体,粒集体,我们只能是视模型不同采取不同的建模方法.比如用多边形来模拟(这也就是最通用的方法,3d图形学呈现给建模者最常用的手段),以产生不封闭的mesh model(surface model),或封闭的solid mesh(surface model).基于数学和几何的方法(有些“过程模型”并不能被在一个3D编辑器中可视化地建模,而只能通过程序手段被“非可视化地产生” – 分形地形,凸凹贴图[2. (实际上,为了方便讨论,以上我们只考虑了静态的3d图形的生成这种情形,而没有考虑3D动画,贴图等)]等,),或基于物理的方法(比如,模拟粒集体内部动力,借助这种物理上的动力逻辑,来表现粒集体最终呈现的动画效果).

联系中的主体 联系 建模者,可视建模 利用3D编辑器制作mesh models[3. 3d建模,就是用计算机完成的“扎纸风筝”活动.] Xx xx 程序员,不能被可视建模 利用对应学科的方法,产生对应模型.

模型被产生的时候,往往是极度不规则的多边形集合,比如用数字设备捕获的3D模型(扫描设备，先扫描出一些点，然后从这些点构建一个mesh,此时只粗糙地捕获了大量点,及其构成的面),如果要被渲染[4. 注意,硬件只提供机制,在硬件级并没有一个渲染的概念存在,硬件只是是渲染的设施,渲染只能是高级软件概念.](通过软件接口来渲染 ------ 接收原始的顶点数据转化成它自己看起来规则化[5. 规则化是指，对渲染这种顶点的renderlib来说，它能接受这种renderlib的rendering]的多边形以进行稍后处理[6. 这是渲染管线的第一步,接收多边形数据,并初始化,在renderman中是mircopoly,但是我们称,用了多边形的方法来解决图形问题的方法为真正的计算机图形学]),必须要进行“规则多边形化”(往往是三角形化,因为较之其它多边形式,渲染三角形往往有它的独特优势),这就是计算几何中常提到的“Tessellation”和“Triangulation”,这些算法往往是硬件的任务,往往被集中在HAL层的驱动中. 还有一个任务是三角剖分。
非设计者支持的建模都是过程的，过程贴图，过程物理，过程曲线曲面，等都是基于程序的，动画也是一种过程动画，但有一定的可控性，基于keyframe的动画给予设计者一些机会指定关键帧。设计者可以指定在什么时间，以什么关键状态（动画的位置，速度等）表达这个动画，然后，系统（可能是程序上的渲染器）会自动生成帧间的内容。而基于物理的动画是一种非keyframe based的，自动的动画。

对于渲染器,如何处理它,怎么样处理更有效率? 稍后，我们会谈到软件的渲染系统，比如windows的dx runtime 对于硬件,如何与硬件加速合作?图形学还必须解决如何利用硬件来直接完成3D图形存储(显存缓冲)及表示(Tessellation),及其它高级处理(比如,如何渲染三角形[7. 这往往是一些算法,比如变换,这就是T&amp;L硬件的使命,解决如何渲染,是图形学软件与硬件系统的合部使命,硬件也可称为渲染器,当然,这个概念是大的,包括前面讲到的:“在软件级或硬件级处理对三角形的存储与表示”,往往渲染就是指硬件渲染的前中端.后端往往是硬件渲染器中的光栅化和光栅器.]),为了达到这个目的(效率,与留给各行各业图形工作者更好的工作手段,当然,对于硬件厂商来说,更是为了赚钱),甚至采取与CPU一样的“计算(procssing)”方式.这就是GPU.GPU的出现,给以了图形编程者“直接控制硬件处理顶点和像素[8. 任何渲染,都是这二个东西在前后二个阶段的合作结果.都可分为这二个要素.都可只在这二个阶段完成.],影响硬件渲染的方式”来解决一切图形问题的能力(而以前“仅仅面对一个黑盒,被动地给以这个黑盒顶点和多边形数据,设置黑盒的渲染工作,等待渲染结果[9. 黑盒较之顶点像素级处理图形问题的方式,最大的区别就是,前者在一个大层次上(三角形处理层面)作工作,由于层次太高级,所以解决的问题有限,而后者,可以在低层次上工作,这种工作正是硬件的工作方式,因此效率将不是问题.更更重要的是,这句话隐藏着:这种编程可以控制硬件作一切事情.当然,这其实也并没有导致开发者的复杂度的提高,它只是把问题转移到了一个层面.我们后面会讲到.]”的方式一去不复返). 对于用户来说,要求最终的渲染结果和图形产品具有实时,可交互的结果.而这,给编程者(解决此二类要求的实现的人)带了大量新的问题,(如果说普通渲染仅仅涉及到数学和几何上的变换知识就足于表达出一个静态渲染结果,那么)这里面大量用到物理和物理模型上的知识.
我们说real time rendering,是因为还有offline rendering,在现实中,rendering的要求是多种多样的,有时只是为了产生一些照片级效果,有的反其道而行之,只是为了产生一些非照片级的特殊效果(比如卡通渲染),作为用3d知识解决的渲染,当然最主要的,还是为了产生一些3d效果和3d场景的东西,比如工程作图,CAD,3D游戏.当3D为了加入实时和可交互的效果时,它就实际上有点torwarding VR的味道(因为它至少是4d的time-space)了.而不仅仅就是渲染出来就了事. 这里面会有什么新的问题呢?比如碰撞,如果仅仅是查询相交,就是计算几何里的数学知识,那么就不需要基于物理,如果需要碰撞反应,那么就需要用到基于物理与力的知识了. 阴影的产生,如果是由别的物体给予的,那么必然涉及到全局照明模型(因为它考虑了光的拆射这种物理现象),取决于解决这种实时问题的手段不同如果是基于图像的阴影,那么就不需要物理(这种模式只是从数学上就可以达到). 所以，不同的渲染效果上的要求，实际上，带给图形学要解决的任务是呈指数级别递增的。
最后,留给程序员的事,实际上,是以上所有这些知识的综合.

**3d render **

前面我们说了2d drawing的原理，实际上，3d rendering(2d是表现图像，3D是表现视频，而且是可交互的视频)，是另外一种”2d drawing”，依然需要用户提供建模数据，然后，底层的硬件通过上层软件的runtime(或直接控制底层硬件，而不是使用定制了硬件功能的软件runtime系统)来渲染这些建模后的数据，完成图形学的任务。

于是出现了3d rendering的概念.图形硬件系统还是原来那个图形硬件系统.只是drawing变成了rendering,因为我们依然能完成没定制新算法到图形硬件系统之前的那些事情.而现在,不过因为我们能干一些额外的3D的事情,所以,drawing变成了rendering而已.图形硬件系统不需改名为3d graphic hardware,而只需改变3d render,人们就能知道它是什么意思了.

那么，从drawing到rendering，到底在硬件上发生了什么样的改变呢？

首先,在历史上,最开始是2d显卡,当时的显卡,只是一张不具备处理芯片,不具备显存,仅具备基本光珊功能(ramdac)的东西,在这个基础上,提出了一系列算法,比如设备支持什么图形格式,如何画线,如何填充,线条如何反走样,,jpg压缩,位图blitting,透明混合,,clipping,如何光珊化等技术,这就是图形学算法要解决的问题(完成这些算法的实际绘制的工作全部由CPU来完成,在现代OS上表现为类GDI这样的东西,它不是一门语言,而只是一个runtime,一种CPU上能编程使用的c语言接口),.

所以,我们往往将PC的显示硬件统称为“硬件图形系统”,它包括显示卡,T&amp;L,纹理压缩器,显存缓存,光栅器等等.虽然硬件图形管线提高了图形数据处理速度,但是由于固定图形管线在顶点级只能实现固定的光照模型和坐标转换,在片元级只能实现有限的几种颜色和纹理映射,不能满足人们对更真实的场景和更绚丽的动画效果的需求.(当然,此时的显卡并没有显式的顶点与片元阶段)

实际上,它解决的,是最基础的功能[10. 对于OS来说,可以靠它显图形,提供给用户一个GUI,显字体,对于用户来说,可以用它作基本的画图.图像编辑.字体技术,实际上也是计算机图形学的应用.],因为任何计算机软硬件界技术,在如何实现的层面,在一开始,都是解决算法的问题.这个时期的计算机图形卡,只是一张普通的显卡.远远尚不具备后来说的"管线".因为此时的图形应用很单调,人们仅仅需要一个可以显示TUI(text user interface)或基本640*320的GUI界面的"能显的卡"就可以了.(图形界面跟文字界面不一样,因为图形界面必须要出现显存,或内存当显存之类的逻辑.因为彩色的图形界面,是用位图映射和颜色系统得来的,故这里需要二个技术,1,颜色系统[11. 阴极射线管可以显颜色,液晶可以靠温度变化显颜色.文字界面时代,文字可以变色,但是在图形界面时代,必须由OS弄出一个软件上的.颜色系统来.,它跟2,位图映射到显存.便产生了GDI这样的东西即soft render.].2,位图映射到显存)

真正的计算机图形硬件只有二个,一个是CRT显示器(当然,其它显示设备也是一样可以拿来被举例的),一个是显卡中的光栅化部件.围绕这二个硬件基础展开的“编程”与“问题”,才能被称为计算机图形学.它是现今我们称为“计算机图形学”的很小的一部分,也是core部分.

但是历史是发展的,出现了3d的应用与需求[12. 与2D实现的算法相比,3D实现与算法,需要强烈用到数学中的线性代数等.这是由3d问题本身决定的.除了加入很大成份的几何处理之外,还在原来2d解决过的那些问题上,比如位图映射技术上,加入了“纹理”,“纹理压缩”,“纹理映射”,在画线反走样技术上,加入了] (图形领域是极其多变的),而实际上,有时即使是2d应用(比如图象制作),也存在特殊性的应用场合,需要更好的算法与效益,当人们需要"更快更好"的显卡的时候,这就要首先解决帮助软件(任何显卡相关的软件问题都是渲染问题)进行加速的问题了(加速不仅是加速,而且是提供新的算法).除了纯粹的基于CPU的软件编程层面的解决方法之外,另外一个最基本的指导思想,就是从显卡本身着手,即从硬件着手,考虑如何将CPU软件级的算法提到专门硬件上来这个课题.这可以在芯片制造技术上着手.比如提出新的图形加速硬件和子硬件，改变硬布线增加功能算法(比如,T&amp;L这样的显卡部件,这导致了该部件作为显卡的"加速器[13. 事实上,当时这些图形卡往往价格十分高,而且往往用在高端工作站上.直到一家3dfx公司的努力,才将这些卡以低端的价格转移到PC上来.]",这样的显卡就成了加速显卡,或整个显卡作为CPU的协处理,在这个时期和这个时期之前,称为传统渲染或固定渲染)或,利用软布线和微指令等技术,使得芯片本身可编程(比如利用流处理思想,这最终使显卡变成了"流处理器",即GPU).所以,历史上,显卡存在二个历史(甚至后来出现了物理加速硬件).

第一个历史阶段,即,利用传统渲染手段(显卡作为PC架构中的附属和协处理身份,此时的显卡,有一部分还是借助CPU一部分计算能力来进行3d工作的,比如早期显卡靠一个T&amp;L进行顶点变换与光照,另外一部分是geforce256这样的,基本将所有3d工作搬移到显卡上,这样的显卡可以产生实时的光影效果,因为它一切都在硬件上产生,已经具有了协处理功能了.因为它已经具备初步独立CPU的功能.)的时候,板卡所有的渲染功能,包括硬件加速的,和软件上能扩展的,在编程上都表现为"向DX这样的runtime提供接口,使DX具备向它的使用者提供一个渲染状态机",使得他们能向"一个具备固定渲染流程的状态机提供几何数据和像素数据",驱动硬件显卡工作的目的.
**超级3d render : gpu **

要谈gpu这个全局的东西,就必须综合联系到图形硬件,图形厂商,图形学发展,图形引擎,图形应用,等等

再后来就是出现显卡向GPU的转化了.人们发现,图形领域太多变,光是在硬件上作如上这种加速改变进行适应图形应用是远远不行的,甚至都适应不了软件级DX的变化要求(显卡能在软件上提供什么样的功能,在一定意义上是种"硬件功能的规范",而这个规范极不统一).

这就是第二个历史阶段.它通过改造显卡协处理器,使之变成流式处理器(关于流水线,是超级CPU矢量计算机,等制造的相关课题,比尔盖次对流水线这个主题有一个关于螺丝钉的著名话题),具备可编程能力.
第一个商用的,硬件顶点处理的显卡是1999年nvida推出的geforce256,此时,它被称为GPU,这个命名是为了跟以前的“光栅显卡”分开.
当然，使显卡具备可编程能力，只是一点，最重要的一点，还是统一所有CG算法的硬件基础，使所有算法，工作在顶点级和像素级（硬件工程中也存在着抽象，硬件工程师们发现：图形学在最低层，最终，所有的图形动作，都要归结为这二个部分）

这就是GPU出现的意义所在。这种改变，在DX下编程，你是感觉不到的，因为DX是程序上的东西，它可以封装硬件的改变。但从此，我们多了一种手段：在DX下进行shading programming.

这就是采用可编程硬件带来的影响,DX这样的面向CPU的runtime,(以程序员的眼光来看)就可以在DX里面内置一个"虚拟机运行器接口"的东西,执行一段硬件能执行的程序,

制造一个中间层(可定制如何调用硬件功能的层面,这个层面是十分需要的,因为如此一来,程序员可以控制固定的协处理显卡"管线"中的某些需要定制其工作逻辑的能力,比如vertex process阶段和pixel processing阶段)而且,所有的书写shader的工作,也可以用CPU上技术已经成熟的高级语言来完成.

所以，shading programming,也是一种GPU上的通用编程，不过，它仅仅面向处理图形而已。
**超越gpu : 从图形处理到通用处理 **

作为一个虽然名为GPU但实际上是个CPU东西的东西,不要被它里面有 vetex processing,pixel processing这样的东西给局限了.实际上，GPU可以作通用计算。

GPU,正如其名字所暗示的,它被设计成“关于图形的处理”部件,但其实,当它一旦是个vector processer之后,它就已经是个GPGPU了,即PC体系上的第二个CPU.相比于CPU能表达的“软件”,“编程”,与问题体系[14. 如果假设CPU是跟GPU一样的图形硬件,在CPU上发展一门“图形DSL脚本语言”,就相当于我们现今常常谈到的shader语言了.],GPU有它自己的一套理念与相关体系.在这个意义上,GPU除了可以处理传统上关于图形的部分(结合CRT和光栅化设备) - 比如在几何上表达用顶点描述的几何体 – 以外,但其实它还可以在物理上表达光,力,这样的东西.因为它是个vector processer,它能很好地用于处理所有这些关于矢量与浮点的逻辑.我们要做的,就是把GPU看成是跟CPU一样地位的东西.把它看成大意义大方向上“数学,几何,物理”的科学处理器.然后考虑它被用于处理图形,和它其中的基本渲染硬件(渲染的后端,即显存,光栅器)合作用于图形学的那些机制.

总之,GPU主体不光是渲染的后端,它其实是一个通俗上的渲染器(还可以处理变换,投影)和科学处理器.
**小总结 **

GPU是第二个CPU,它能利用几何上和物理上的知识来解决问题.这跟CPU能解决“为硬件提供一个软件平台”,“运行程序”的能力(这些事情是面向PC应用的,所以是通用的东西,而GPU上并没有必要发展OS,它只被设计成接入PC,利用CPU已经制造好的OS等基础设施,处理它自己的事情,即图形的问题)是一个道理.而CPU和GPU又是天然结合在一起的(CPU已经完成了的事情GPU就可以利用它干自己的事),故,GPU可以利用CPU的能力接入PC,但其实它更在行(hang)于处理自身的那些事情(图形,或其它矢量逻辑的事情).

所谓计算机图形,其实就是CRT和光栅设备能完成的那些事儿,所以,GPU全部能力中的用于跟CRT和光栅化设备协作的那些东西,就是真正的计算机图形学的概念.而我们现今所用的“计算机图形学”这个概念实际上是个大材小用的概念.它实际上更适合被称为VR,因为它不光解决了从几何上表图形对象的所有事情,(为了进一步的真实感与交互性)还解决了从物理上表动画,效果等东西的内容.

GPU用于图形学的东西有:表几何体,表力,表光与辐射度,表材质,表材质映射,表模型,表碰撞,表场景,表特效,表粒子.所有这些,构成了计算机图形学.

所以,计算机图形学是有层次地利用GPU能处理几何和物理现象的能力所构成的,是关于(GPU能力中表)几何体的,(再以前者为基础加上GPU能力中表)物理特性的.上帝说,先有光,后有XX,而计算机图形学并不是分散无序地利用GPU能力.而是迫于图形学的最根本需要,先表几何现象,再为了真实感,在物理上表达一些东西.(这就是GPU通用处理用到几何和物理,以及GPU用于图形学也用到几何和物理的分别所在)

以上,就是计算机图形学最科学的定义,我们的章节组织,与学习曲线就是根据这个而来的.

>实际上,计算机图形学是个学科密集的领域.因为解决计算机图形问题的过程中,人们用到了数学(线性代数,数值计算方法,数学分析),几何(计算几何,射影几何[15. 在特定的的渲染时机,比如照像机到显示器时.]),物理(光,与色彩),生物(人工智能).所以,实际上,一开始并没有一个所谓的“计算机图形学”这样的学科,只是人们利用以上知识来解决计算机图形问题的过程中,把出现的问题跟相关的解决之法综合起来形成的一整套理论,叫做计算机图形学.这种现象也导致了另外一个结果,即,图形学硬件跟软件高度统一,对于编程者来说,它如果懂了软件,就几乎懂了硬件的编程.从来没有一个领域,如此高近度地软硬结合.并且涉及到如此多的算法[16. 算法有二种,查找,排序这样的算法叫通用算法,图形学相关的这些的算法叫专用算法],它根本不像其它逻辑一样在CPU上进行扩展,基于CPU上的编程只是给图形编程在软件上提供一个可编程运行时,其它更多的主体领域在于硬件相关,算法相关的交叉点,因为图形学不是传统的软件开发意义上的软件逻辑.因为图形学在根本上是用以上几门学科直接促成的. 你可以拿它跟网卡和TCP/IP的关系进行比较


>此时的cg 所以,其实,计算机图形学是一个极其被错用的词,它实际最早就是指这些有限的东西,最后才是指代一个大概念,即面向现代vector processer的整个软件,编程,与问题体系(而显然,这里面不仅仅是一个“关于图形的处理”). 现今,我们把几何上和物理上的这些东西,称为“计算机图形学”.但实际上, GPU能处理的几何上的表示,已经足够被称为”计算机图形学“.如果加上物理学，AI，那么这些远远超出了图形的概念,(现在的图形学)它更适合被称为“VR“,所以“计算机图形学”，更适合被称为“计算图形”，泛指一切支持在计算机上显示图形的学科综合体。

Xx
如果这本书的题目叫Learning vr programming?其实也适合,这是为什么呢? 建立一个“图形”意义上的东西并不难,难得是建立一个正确的“图形世界”(如果要求是正确的,那么图形远远就不止是图形意义上的了)当然,我们讲解的3d可以不要碰撞什么的,但是如果要建立一个视觉上正确,功能基本完备的“可交互式”“3D”,即“VR”,那么我们就要学习physic-based的一些东西 相应地,我们不要真实感,那么像材质,纹理,打光(其实更准确的说法叫照明),都可以不要. 图形卡只分顶点处理和像素处理这二个大阶段,这似乎暗示着只有几何学和图形学(颜色硬件和视频缓冲相关的图形硬件相关的软件图形逻辑)有关,但其实通过直接“操作顶点”和“操作像素”得到的“3D”很不强大,如果想得到“VR”,就必须依赖物理技术,(不是所有VR的事,比如碰撞,都可以在顶点级或像素级得到,实时意义上得到)据说,最新的nvida有pysiX，DX规范规定有物理,这说明,物几数三大学科是VR的三大基础,这远远超出了原来那个所谓的“顶点”,“像素”所指的图形学和图形学,而是ＶＲ学了.
下面，我们就可以来看一看3d rendering的整个过程了。

**3d rendering,common 3d rendering前端与gpu 3d rendering前端 **

>这段去掉： 如果PC装配了GPU,我们说它有二颗心,说GPU是第二心,是因它它的确可以跟CPU有得一比,但是它2pu专用,是因为这个2pu中的gpu成分很严重,它的指令和机制全部是高级的.所以使用这样的asm其实跟使用hlsl是一样的.Hlsl增加的仅仅是一些语句上的手段.这些语句并不关乎太多2PU能处理的问题上的语义. 高级shader语言,其实更像高级化了的shader会编语言

前面我们谈了普通3d render和超级3d render - gpu的出现，是不断增加CG算法到硬件的结果，然后进行统一到顶点级和像素级的结构，那么最初，有哪些算法要加入到3d render呢？
**通常有可能加入到普通3d render[17. 注意,我们这里谈到的3d render不限于指dx runtime这样的东西,它只是开发层面在软件意义上的一层包装层.]中的新算法,伪硬件**

**硬件加速就是指：对应于用纯软件实现图形学程序，硬件加速往往是软件上封装硬件的功能来实现图形学抽象和程序，受硬件支持，由硬件来完成运算的。**
在普通3d render时期，这个时期，一般图形程序员主要利用soft render，cg界的新算法要么实现在加速卡，在软件的soft render,比如dx上透露出一个接口，或者一些高级图形程序员，可以在cpu上定义自己的，高级的软件上的CG算法。

对由GPU组成的新图形硬件系统的研究,由于统一了硬件基础，如果不利用softrender方式.那么，所有高级图形程序员定义的CG算法，就可以全工作在GPU上了，

所以，这二者其实是统一的，只是算法工作在common 3d render还是gpu上的区别，CG界用来解决问题的算法还是那一套同样的算法。

Common 3d render和gpu 3d render可以允许你定制一切cg的算法.但3d render是一个不跟 2d drawing一样的地方(比如2d经常是图像数据,它不需要顶点和建模,而3d图形的表示,强烈用到顶点,深度,等表达的模型,甚至是纹理),它解决的算法和问题是有特定流程的(一般是先建模后贴图打光),这就是流水线.实际上,GPU给以用户新加速算法,就是因为其内内置了这些模型,纹理相关的硬件单元(比如对于模型来说,是变换器,对于纹理来说,是纹理单元,对于光栅来说,是rop单元),当然,这并不跟它能用于GPGPU有矛盾.

这就分为以下几大级别:工作在变换级的,工作在几何级的,工作在像素级的算法
当然在2D时代,只有像素处理,而没有一个顶点处理.因为顶点那是3d为了表深度而加的,是3d专用的.

图形处理硬件分顶点和像素处理这二个阶段,这说明,硬件只支持这二种元素(其它的都是程序的东西),它只提供表示顶点的显存,能高速处理顶点的能力,和高速响应你的指令形成的算法的能力,(根据你提供的指令和算法)在你指定的某个时刻光栅化掉顶点,此时顶点的位置信息全丢掉,显存中没有顶点,只有像素,(硬件直接支持多少多少格式的像素是硬件的特性),像素中的字段有颜色,光的信息,但没有位置,深度信息.硬件只需要这些作最终的显示.
Pixel shading前已经丢掉了所有的vetex信息,变成了不止三个的pixel,每三个vetex变成一片pixel

Vetex shading就是在位置信息没丢之前,利用硬件vetex processing的功能变换顶点,pixel就是在顶点被打成pixel之后,对这些pixel在硬件级进行vetex processing进行,它不是“对硬件进行编程”以实现什么东西,而是利用“硬件处理能力”进行的,“高阶处理”,就跟软件能完成的一样.而且在相仿的层次,所以shader的引进,并没有给d3d这样的编程带来特别大的复杂性.
初会顶点 在2d中是没有顶点的,只有像素与像素处理 dsf 初会纹理 在2d中只有平面图像,没有纹理与凸凹
1．3d render 前端:变换器 这个阶段统称为vetex processing阶段. 在加速器时代,顶点变换,往往由T&amp;L来完成,在软件runtime级,往往用于实现流水线,比如DX的流水线. 流水线各个阶段的投数据时机的特点 The rasterization module is the 3D renderer in Direct3D. 渲染器的瓶颈在哪?在它跟CPU的交换数据处
2．3d render 中端:几何器 这个阶段统称为geogemetry processing阶段.往往是新增的,XP的dx sdk并不支持.

xxx

在GPU中,为什么设定顶点处理和像素处理这二个东西呢?为什么不是其它东西可以定制与编程? 对于渲染器,所有的一切,是解决如何用几何元素(可能是点的层次,也可能是更高的,比如,线,面层次,在用户建模级,或算法实现级,)来构建几何体的问题.这就涉及到几何体对于渲染器表示细节. 但是对于硬件,永远是从最基础的顶点与像素着手的.

可以用点,可以用mesh,patch,或者曲线条逻辑(这是一种数学逻辑).

3．3d render 中的后端:像素器 这个阶段统称为pixel processing阶段. 要明白,顶点跟颜色是不一样的,像素跟顶点和颜色也是不一样的概念.流水线处理颜色的过程叫shading,处理顶点的过程叫vetex processing,处理像素的过程,叫pixel shading.

着色的完整说法是“为像素计算颜色”,以便渲染(可以称为渲染前的渲染) 像素是一种没有位置信息的顶点.
**3d rendering后端 **

3d流水线,决定它是用统一的方式,比如用顶点和纹理这样的东西来绘制所有的图形逻辑.render绘图原理,纹理映射与顶点矢量,等3d render的工作方式,给了我们一条非常好的,关于学习3d的有效支节关系.

故,对于渲染器来说,只有二个东西,1,多边形渲染(如何定义pure图元和with纹理映射的图元),2,几何变换(如何进行变换),这二点才是一个渲染器关注和3D及3D编程要解决的问题的.其余的工作(实际的,具体的渲染工作),就交给渲染器来完成(这之前是创建设备,定义一些光照逻辑,然后开启一个场景,进行渲染等等,你所要做的,仅仅是定义,render支持表面绘制,clip plane,双缓冲,等等,形成动的画面..注意,这里动的画面,只是d3d渲染时采取的一种底层缓冲策略,不是动画,比如骨骼动画,那是3d的内容).

首先,d3d render 眼中,一切都是图元(所谓模型,只是用工具做出来的,但最终要转化成d3d眼中的图元,只有图元,才是d3d直接能渲染的,模型中的动画,骨骼数据,全是图元的变相形式),而图元,只是一些位置数据.因此,一般教科书,只讲图元及其渲染.

这里面的内容有:定义顶点格式,(数据驱动编程:d3d为我们做了很多工作,程序员只需)填充具体的顶点数据(用“使用顶点缓冲”或“带有用户内存指针的索引图元”).最后,你就可以渲染不带“纹理映射”逻辑的单调图元了.

pipeline流水线决定了怎么样变换.最终经过一系列变换到显示器的过程.它处在rendering的后端。

**应用程序阶段和工具阶段 **

在Rendering的各个层面上，在最低处，是图形卡处理的像素和点，在最高级的应用级则可以是任何程序上抽象的事物，在它们中间存在大量的中间阶段。用这个观点可以解读CG所有层面上出现的概念和算法，并把它们连成一条线。这个层面是这样的:

数学，是一切科学的基础。 计算几何，计算物理，计算智能，为下二层提供科学手段和科学解释。 modeling 层，用户在这层提供渲染用的数据。 Rendering层，完成渲染的任务。 然是app层

这些层面在具体各层的各种任务中会不断交替。

这些可用来解读CG的变换，动画，物理动画，rendering pipeline,关节动画，地形，lod等等。

比如:

顶点和顶点的属性是CG级的东西，顶点表现的“三角形”是应用级的概念。多边形图形学使用三角形是应用级的策略，特别是三角形以实体或其index为存储领接抽象等手段是应用级的策略。对于三角形的处理（face的三角化）是CG的事情，然而对于三角形的进一步处理，又是应用级的事情。 Scene是应用级的概念，不是CG上的东西，虽然它的底层最终要深入到CG的空间划分和管理方法(scenegraph和bsp等划分算法是没有必然关系，bsp是一种算法，而不是一种设计,scenegraph也是一种算法而并非一种程序上的抽象)，在CG级是如何建模物体和如何处理变换的逻辑，并没有一个空间，并没有必要表现一个“SCENE”。 包围体常被用于CD与CULLING，包围体是应用级的策略，在CG级是统一的对convex的讨论，学习CG，就是分清诸如这些CG下的数学，几何，物理现象及其在APP级的一系列对应。 LOD:lod与视点有密切关系，LOD是应用的东西，化简和相机模型是CG级的东西。 Collsion Query只能在三角形级确定与哪些“三角形”以ray cost等方式碰撞，在应用程序级才能获得Collsion Query与哪些“物体”发生碰撞的效果。
**3d开发的特点 **

**由于图形硬件架构的特点是GPU越来越取代cpu上的软件编程,故现今图形开发的特点是shading language用来负责制对cg算法编程,而软件上的引擎用来作引擎编程.**

3d render,gpu,gpu上的通用计算，和3d rending的出现,所有这一切，是一种什么样的意义上的改变呢?对于编程者,有什么样的新要求呢?也就是说,传统开发(pre offering the graphics hardware system vetex),和基于GPU的开发(online offering the vetex),是不是一样的呢，哪个会更复杂一些呢，哪个需要的知识多一些呢？(其实在算法上是一样的,只是在提供用户数据的手段的时机是不一样的[18. 可以投预生成的表达为顶点的数据,比如模型,但对于GPU编程,更适合投一些过程生成的模型.])?

>rendering是软件的概念,硬件级并没有一个render!!它们只是分散的各负其责的图形硬件而已.只有当它们被组织起来了,才能形成一个rendering过程.而这,只能是软件级的动作.这其实就是在谈CPU和OS关系时，我们提到的机制与策略的关系。在CG中，机制只有一个，那就是GPU，而策略有二大套，即soft rendering和shader rendering.，它们本质上是一样的，只是在某一些事件发生时，安排的机会不一样而已。本质上，都是对顶点和像素的处理，只是处在高低不同的层次上。 在下一节，我们将先选取soft rendering,比如dx 的高层模型来谈。

在3D界,人们都在追求着一种"dos like"developing,即崇尚dos下开发游戏时的那种"直接访问硬件的方式[19. 但这又与GDI-like方式,将硬件交由windows来管理的方式矛盾,dx就是doslike和gdilike的拆中.]"(这主要是基于3d软件效率的考虑),加速器时代和加速时代之前的针对固定渲染管道的编程,特别是GPU vector processing时代给GPU定制指令的编程方式,其实都意味着,3d开发中其实软件和硬件的那层界限很薄(给于开发者有意义的可复用中间件抽象),对于开发者来说,任何基于软件render层面的高阶开发,其实跟发明一个render差不多,都强烈地用到"图形算法",强烈地用到曲面数学,线性代数,解析几何,计算几何等东西.(除了数学,几何,有时甚至是物理知识,所以人们说,3d开发其实是一个学科综合,知识密集的领域,,游戏开发就更是如此了)在文章的稍后,我会介绍与3d开发相关的数学体系和物理体系.拿DX来说吧,它几乎是最新的3d和3d开发规范,比如,shader也是它首次倡导出来的.比如它有立既模式(由厂商在其驱动程序中来保证一个hal层,对于硬件没有的功能,windows在底层维持一个模拟层,hal和模拟层构成dx中的立既模,这样对于保留模式的DX,它都可以提供一致的硬件接口)和保留模式二种.但即使是高阶的保护模式,虽然它也有内置的变换(render后期的视口变换),但对于程序员来说,也要写自己的变换(通用变换,对于程序员来讲，矩阵的求值，它建立在“理解经过了什么矩阵堆栈”为基础原理).因为上面说到:3d领域其实是一个非常"专业"的领域,对于程序员,无论在硬软件的哪个层次上进行开发,最初都要涉入到图形算法.正是由于处处需要通用变换和深入到图形算法,所以需要shader.shader的引进,对于3d开发,没有引进任何更多的复用性.它只是假设,任何一个3d程序员,必须是一个了解图形学算法的专业程序员.至关重要的一点,是要注意,管道,流水线如此类概念,其实硬件是没有的,只是3d render software层面的概念,它只是一些硬件设施的相关机制的保证,这就跟CPU和保护模式的关系一样,能说x386是保护模式的CPU吗,我们只能说,CPU在硬件上提供了这样的一套机制,这套机制被用来进行软件的"保护模式"支持.而显然,保护模式是软件层面上的意义.推而论之,固定管线,可编程管线,其实只是软件层面的意义.故shader是硬件部件(的一个过程,可定制指令的过程,是一个硬件上的意义,GPU的出现,整个地取代了原来显卡在硬件上的意义,使之变成了vector processer而具有shader部件),dx在软件层的shader层是不是为了取代什么,而是全新设计的一种架构,它目的旨在让程序员在软件层面对硬件提供可定制的指令.所以,所谓可定制指令,其实正是"针对硬件写程序",但是上面已经说到了,由于3d开发是一个专业领域,所以图形算法,在dx层这样的软件runtime层和在硬件处理的shader部件的复杂性其实是一样的.为什么这样说呢?3d领域是一个非常"specialized"的领域呢,因为这里不外乎图形学的算法,即数学,物理,几何知识密集的这个地带.这首先要讲讲3d render 的流程并详细导出这里面的诸多算法.为了简化起见,我将分步讲,先说关于几何信息的,然后讲关于颜色信息(颜色是一种光,是物理现象)的.以及shading,lighting,foging,textures sampling的,在现实中的render中要注意的,这几个步骤往往是平行的.一般的引擎是基于多边形的引擎(从几何出发),当然也有基于ray tracing的,纯粹从物理出发的.我们这里讲普遍的基于多边形的.首先,用户通过Modeling(可能是曲面或面片,但更多的是poly的)或程序动作的loading x文件[20. 可复用的3D引擎或其它中间件,给3D开发带来一种数据驱动的编程模型],载入一系列顶点定义,然后引擎将其micropoly化,或faclet化,这称为dicing(一般dicing为三角形网格),然后进行hiding 不可见面的算法,到这里时候为止,整个模型是线框信息(纯粹的三角形网格信息),然后shading(shading翻译为"画阴影其上",,我们把其理解为为了不致于使模型是线框信息,而进行的"以表面三角形为单位的",贴上阴影,表现其初步材质性质 - diffur反光特性的过程,注意不是贴图texturing,而只是shading),比如flat shading算法,这使线框网格变得solid,还比如其它一些smooth solid化网格的高级shading算法,会使模型具有更真实的现实效果,此时,模型已有一定的真实感了(如果是通过曲面技术生成的网格,其本来就会显示出一定的平滑性,shading过程对这样的网格效果更好),注意此时只是表现材质,为了追求更好的真实性,还要加入贴图,即来自位图上的颜色信息,texturing其实也是一种shading(shading是一种三角形项点材质反光信息的插值),不过它不是插的材质反光信息,而是颜色数据.如果不是程序员的眼光来看,比如以一个modeler 艺术家的眼光来看,shading往往是定义一些材质,texturing往往是制作一些UV.以引擎实现的眼光来看,它们要实现平面贴图,BUMP贴图,之类的算法艺术.然后就是light.注意lighting不是定义材质,材质只是反光特性,始终要注意的是,到现在为止,只是顶点(信息)的处理(比如位置,材质,颜色数据),这也就是vetex shading阶段能做的全部事,和dx 提供一个vetex shader的意义所在和其所针对的渲染阶段.而不涉及到任何像素级的处理.但是对位置,材质,RGB信息的处理,显然是为了最终像素的处理作铺垫(材质,RGB,lighting最终决定了这个顶点在被scaning into pixel后形成的pixel 信息) .等到作像素处理的时候,引擎就进了"pixel shader",一般地,如果谈到这里,就到了渲染的后期阶段.算法在这里的也比较多,请自行研究.

>CG程序在逻辑上分为渲染的逻辑和建模的逻辑。 渲染在程序上，就是rendering algo和rendering abstract。 计算几何算法涉及到的细分（地形等事物要用到），插值系统与动画（动画事物要用到），物理系统与物理模型的建模(水，刚体与粒集体，柔体事物要用到)，这些，就是CG中的建模要涉及到的内容。 所以，Rendering并非代表整个cg，它是有限的rendering algo或基于algo之上的absctracts. 比如dx仅对rendering有支持，而对如何建模没有作任何抽象(d3dx中用一套对动画的建模程序)，相比之下，hlsl就是对渲染和建模没有作任何程序上的抽象的机制，它允许一切由用户定制。 而建模部分，涉及到的数学物理知识最多，而渲染部分，往往由DX运行时等提供了，所以，3D开发，是一个除了基本渲染逻辑，就没有任何更多支持的领域。你依然要掌握大量知识（甚至是数学物理上的）。

**Dxruntime的根本任务 **

[21. Dx不是render本身,而是render lib,render runtime,render本身只是硬件的作用.作为render lib,它里面一定提供了对modeling,rendering的同时支持]

dx只应提供buffer等硬件资源(比如透露一个硬件上的渲染装置给用户,比如控制渲染状态的一些STATE),不必提供顶点,等等东东,这些程序员需要自己动手的东西,这就是d3d core的任务,D3DX相对来说完成了很多其它程序上的任务(CORE的任务只应是透露硬件)

然后,其它基于它的,更高级的图形程序或引擎就可以被发明出来了.所以DX相对这些引擎来说,是底层和基础.它提供了最最迫需的硬件加速和最最初级的图形接口。所以是一般图形程序的基础。

必须明白dx runtime[22. 它只提供编程上能用的hardware render core函数，如rendering primitives,texture,light,所有这些函数，等价于”fix line function”]这样的概念:CPU是通用处理器,而GPU,FPU这样的东西,只是主处理器下的协处理器(这是我们的历史与现状).也就是说,任何显卡相关的渲染问题,在编程层面(虽然在底层,os 对显卡driver的封装形成一个类dx的runtime,将图形应用程序转化为显卡硬件功能),都体现为CPU跟显卡结合发挥作用的(以下会谈到GPGPU,其依然也是CPU为主,GPU为主).所以,固定渲染与可编程渲染,一方面指的是硬件上的改变,另一方面,也是其同时体现在DX runtime上的改变.反映到DX这样的库上,它也只是提供一种CPU级的编程接口(dx runtime),使得硬件功能能处在CPU的"编程支持层面"上,以形成图形应用逻辑.这样,底层再怎么样改变,管它是固定管线,还是多道流式处理器,DX只需提供一个共同接口,而且,对于shader程序的接入,也只需提供一个"虚拟机运行器接口"这样的接口,将其导向到流式处理器中[23. 为了跟上并发挥最新的显卡的能力(不可能在dx固定管道上直接增加大量的这些新增功能)，于是人们发明了将GPU能力接上dx runtime，利用的是可编程的办法].当然,如上所说,这些都是在"CPU上解决高级语言编程问题,在DX runtime解决接入的中间层问题,在GPU上解决运行的问题"的体现.
懂DX编程假设用户需要懂图形学算法

ok,我承认我说谎了 d3d帮我们完成了大量工作,我们在做图形编程时总会用到顶点,如果是在GPU上作low level graphic programming,那么这种顶点是属于不受d3d控制的(也就是说,这相当于发明一个software graphic render时使用到的顶点,这种顶点,当它被投入vetex processing时,需要进行三角化,而这种算法需要用户提供算法支持),如果使用d3d格式的顶点,那么用户可以使用index buffer,vetex buffer这样的程序上的高级手段,而且无论是原始形式的顶点,还是以buffer形式组织的顶点,都会由d3d 自动进行三角化. 所以,虽然说在没有一个图形引擎前使用顶点,或在拥有了一个图形引擎后使用顶点,这二种情况看似相同,但其实大不一样.(懂DX编程假设用户不需要懂图形学算法,比如三角化) 而且,这种DX编程,并没有提高太多的抽象,它(没有隐藏顶点逻辑这些关乎图形编程必须由程序员定制的部分)只是隐藏了一些诸如三角化这种技术上的算法.混合了DX的GPU编程也是如此(然而纯GPU的编程就需要处处自己发明图形学的算法抽象)

**Shader programming **

gpu编程到底是提高了抽象度,还是降低了抽象度呢? 当然,新式的GPU能完成的工作,在这个时代,已不仅仅局限于"显卡相关的渲染问题"了,当然,那又是另外一个问题即GPGPU(但是,其在编程上的解决方案跟提从DX runtime这样的手段相似)......

图形编程
-------------

现在,我们能看看什么是图形编程的真正含义了.它不是CG的事，而是CG在程序上的事。 什么是图形编程,什么是游戏编程呢?它跟多媒体开发,3D,VR这些字眼有什么关系跟区别呢?

首先,前面三章我们讲了图形学算法,它的基本任务是渲染3d事物,3D事物可以是简单模型,复杂的综合模型(那种包含骨骼,甚至动画的,比如,.x文件表达的模型[24. 这样的模型,严格说来,是模型场景]),定义3d事物是用户的事情,或DX,OGL引擎提供给用户的支持(从编程看,3DMAX也属此列),渲染则是DX,OGL的事(当用户指定shader assmable,实际上也承提了一部分引擎的实现),既然从编程上看,整个3D最终归纳为定义3D事物和实现渲染逻辑,那么设计一个图形引擎(on top of dx,ogl的实时图形引擎)也就归纳为定制和扩展这二大动作.我们要设计的引擎是偏游戏图形学的,所以跟开发通用CACAD的图形学引擎还是有差别的.

对于用户而言,3d主要解决2大问题,1,表现3d空间本身和其内的各种物体(位置信息,这个完成可以用数学上的方法来解决),即各种物体[25. 关于物体层次,不同的情境下有不同的说法,有时一概是顶点,有时是三角形,有时是网络.有时是全局(光照)]关于如何存在于空间内被表示(及可能以后发生各种空间变化的表示),2,表现空间内各种物体和效果(以逼近现实生活中真实物体效果为目标),即物体渲染的逻辑(可能经过渲染最终被打上颜色数据,可能最终是被打上了光照数据,而这些,无非光照等环境问题和物体本身这二大部分,只有这二步完成了,渲染就完成了,可以送交光栅化了),现实中(比如3d max等用户)采取的渲染的工作模型也像极了现实中作画者的行为模型,比如,它也像画画一样,先素描(建模),再着色(纹理贴图).也打光[26. 没有打光的,光有纹理与建模的世界是不是一片黑呢?这取决于引擎用了什么的默认方式.] (着色和打光,就是渲染了,一切的渲染算法,在总体上是为了解决这二个大问题),然后游戏引擎,或3dmax,只负责回放就可以了.这就导出了渲染.

当然,这只是呈现在外面的特征而已.实际上,对于编程者实现者而言(贴图与打光,都归结为一个“对形成的网格“进行”上色shading[27. Shader的意思是刮胡刀,刨子,焗油,它针对“surface”进行shading.] “的过程 - 这就是也可以用pixel shader来解决的那一类问题”),渲染(它一定是在低部解决了什么问题,实现了什么算法,这种效果能被正确展现,一定经过了图形硬件的作为,或经过了CPU计算),这些也是实现渲染器与回放引擎的算法.

实际上,关于如何表现空间,如何渲染空间诸物体,这些构成了计算机图形学(中的3D部分),而且这二者显然不是没有联系,其中,2是时刻以1(矢量与矩阵.)为基础的(比如贴图也是用到位置信息的,比如纹理位置抖动可以形成水波),1往往是基础,被用于讨论2,实际上,如何渲染,这是一个巨大的问题.在这里,就聚集着大大小小的图形学中的算法们.

一切几何问题来自投影.2D在一线中的投影是1D,3D在二维平面上的投影是2d,我们总是用n+1维来研究n维空间.这就是立体几何[28. 这之前是弄明白2d解析几何先] (解析几何是几何与数学的结合),它的基础理论是投影.而这在数学上对应,线性代数.(向量代数是几何与数学的结合).这就涉及到复杂的数学学科中的的互换(代数系统),也涉及到各种空间的坐标转换.
一切渲染问题,来自光照和物体本身这二大主要因素(还有周围环境),它产生光线逻辑,和颜色逻辑(我们人眼能看到颜色,也是因为物体反映这种光给我们眼睛,光线数据,也是一种RGB数据).产生一系列问题,比如,阴影.光照.

光照问题深刻地同几何特性相关.所以说,3D解决问题的二个方面是互为基础的.

**图形引擎与渲染引擎的区别
**

渲染引擎只完成整个CG中的底层和很少部分，如三角形网格的表示与存储，三角形级别的shading,三固定管道功能的提供（剪才等，当然，光栅化这样的东西已被硬件完成了，一个软件渲染器不需要再完成这些），而图形引擎较之渲染引擎，其最鲜明的特点是它处于整个CG的高层，在软件实现上，它往往基于渲染引擎，提供针对整个场景级别的图形方面的处理，最鲜明的特点就是它往往使用场景图结构和算法来组织渲染和处理。

**图形引擎:引擎只是图形学通用处理过程在程序上的不同发展 **

如果将程序作为一种产品来考虑，那么将包含图形抽象的那部分程序作为产品的一部分，比如作为一个图形引擎，那么这样的**引擎不但是作为开发库存在的，而且是作为图形产品而存在的**。

我们首先要确定问题,我们现在在做的是一个有限设计,设计的引擎是“偏(Pian第一声)游戏的图形学引擎”.而不是3dcad等用的通用图形学引擎.
虽然generic engine和war3 engine都是adv engine on top of dx,我们要制造的引擎是类OGRE的generic engine,还是类war3 engine一样的专用引擎呢?

>引擎OGRE与DX,3DMAX的关系

虽然从程序的观点来看,计算机图形学通用处理过程是一样的,但具体到一个程序,比如它是游戏的,还是编辑器的,还是最终体现为不一样的过程. 虽然OGRE只宣称它为一个图形引擎而不是游戏引擎,其实ogre作为render's render(advanced render on top of dx render),称为一个game engine是一点不为过的(ogre中也有pass的概念,而那是d3d的概念),应该看到的是,市面上所谓的大多数作品,它们使用的游戏引擎往往是一个图形引擎. OGRE中的某些抽象与功能与DX中的对应功能与抽象,有什么联系呢? 如果render hardware只是渲染,回放,而非生成“比如骨骼动画”,那么GPU(的处理能力)是不必要的,就像mdx和md2的区别,骨骼动画的顶点动画的区别一样. dx并没有一个“实现了骨骼动画”这样的概念存在,它只是内置回放这种动画的能力,而不能负责具体“生成骨骼动画”,3dmax才是(它负责填补中间帧),然而IK[29. 简单的骨骼动画不一定要做到ik或cs的bip,只可能会用到constraits或controler,只是人体的动画属于复杂动画,骨骼太多制作动画的时候得手工完成很多工作(当然也有自动化的手段和cs bip这样的更高级的手段),有可能会用到这二大抽象了的高级constraits而已.这都是程序上的事.]作为3dmax制作动力动画和DX能回放这种动画的理论,是统一的. 你会很容易发现,图形学算法中,涉及到如何建立一个类3d max软件功能的内容很多.这是因为程序员和建模者(或者游戏mod),遵循同样的过程(这就是图形学领域的通用处理过程,从编程上来看,都是统一的[30. 无论什么图形引擎，包括DX和我们要开发的图形引擎是基于DX之上的，因此要重复一样的图形学算法的通用处理过程，不过起点不一样，DX要做的要基础一些，基于ＤＸ之上的图形引擎不需要干ＤＸ已经干过的事情。这句话是在说：我们在这一章所有要做的事，就是程序上产生一个基于ＤＸ之上的图形上引擎，而引擎一字，实际上是程序的说法。。-------重现图形学的算法，但是除了ＤＸ已经干过的事情之外。，而有时候，太抽象了的程序，不会直接实现，因而看不到某些通用算法与通用程序上的处理。我们要写的几个过渡版本，就是有意突出那些重要的图形学算法与程序上的重要过程。当然，也不故意发展到高级层面，也不刻意去隐蔽重要的抽象。一句话，最能代表偏游戏的图形学引擎技术以及通用游戏的那些程序抽象，我会写出来(参照wildmagic，我认为那代表了最核心的技术)。]).必定是先(比如地面)建模,然后表达其真感实.Render engine和程序员,都用着同样的算法.处理这些东西.以进行显示或回放.

因为图形引擎都是一个通用处理过程，在不同的层面上的发展，都是程序上的东西。因此这里有必要说明一下，我们的图形引擎处在什么程序层面上。

几种程序上的层次与通用处理过程 cgal,irrlicht ogre wildmagic d3dx d3d softrender

>大而全的通用引擎，与具体的非通用引擎之间的关系 什么是图形学的通用处理过程?从编程的角度看,我们的设计就是再现这个通用处理过程(比如一个像3dmax的专业model editor跟一个图形引擎的model viewer往往都有对模型结构的定义和实现,故,对模型结构的定义和实现就是一个通用处理过程). 无论什么图形程序,都要在算法层解决一些问题,都一般要实现什么功能.考虑问题应从这里开始,因为我们要开始的是一个库,而且这个工作,往往都是基于某种实时引擎,比如DX.OGL
